{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "GitHub ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: do the **take home** exercises in the [DM19-Lab1-Master Repo](https://github.com/EvaArevalo/DM19-Lab1-Master). You may need to copy some cells from the Lab notebook to this notebook. __This part is worth 20% of your grade.__\n",
    "\n",
    "\n",
    "2. Second: follow the same process from the [DM19-Lab1-Master Repo](https://github.com/EvaArevalo/DM19-Lab1-Master) on **the new dataset**. You don't need to explain all details as we did (some **minimal comments** explaining your code are useful though).  __This part is worth 30% of your grade.__\n",
    "    - Download the [the new dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). The dataset contains a `sentence` and `score` label. Read the specificiations of the dataset for details. \n",
    "    - You are allowed to use and modify the `helper` functions in the folder of the first lab session (notice they may need modification) or create your own.\n",
    "\n",
    "\n",
    "3. Third: please attempt the following tasks on **the new dataset**. __This part is worth 30% of your grade.__\n",
    "    - Generate meaningful **new data visualizations**. Refer to online resources and the Data Mining textbook for inspiration and ideas. \n",
    "    - Generate **TF-IDF features** from the tokens of each text. This will generating a document matrix, however, the weights will be computed differently (using the TF-IDF value of each word per document as opposed to the word frequency). Refer to this Sciki-learn [guide](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) .\n",
    "    - Implement a simple **Naive Bayes classifier** that automatically classifies the records into their categories. Use both the TF-IDF features and word frequency features to build two seperate classifiers. Comment on the differences.  Refer to this [article](https://hub.packtpub.com/implementing-3-naive-bayes-classifiers-in-scikit-learn/).\n",
    "\n",
    "\n",
    "4. Fourth: In the lab, we applied each step really quickly just to illustrate how to work with your dataset. There are somethings that are not ideal or the most efficient/meaningful. Each dataset can be habdled differently as well. What are those inefficent parts you noticed? How can you improve the Data preprocessing for these specific datasets? __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "5. Fifth: It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**. __This part is worth 10% of your grade.__\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb). Make sure to commit and save your changes to your repository __BEFORE the deadline (Oct. 29th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Part: Take Home Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# necessary for when working with external scripts\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the documents containing the categories provided\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, \\\n",
    "                                  shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# my functions\n",
    "import helpers.data_mining_helpers as dmh\n",
    "\n",
    "# construct dataframe from a list\n",
    "X = pd.DataFrame.from_records(dmh.format_rows(twenty_train), columns= ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['category'] = twenty_train.target\n",
    "X['category_name'] = X.category.apply(lambda t: dmh.format_labels(t, twenty_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 2:\n",
    "\n",
    "Experiment with other querying techniques using pandas dataframes. Refer to their documentation for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    From: sd345@city.ac.uk (Michael Collier) Subje...\n",
      "1    From: ani@ms.uky.edu (Aniruddha B. Deglurkar) ...\n",
      "2    From: djohnson@cs.ucsd.edu (Darin Johnson) Sub...\n",
      "3    From: s0612596@let.rug.nl (M.M. Zwart) Subject...\n",
      "4    From: stanly@grok11.columbiasc.ncr.com (stanly...\n",
      "5    From: vbv@lor.eeap.cwru.edu (Virgilio (Dean) B...\n",
      "6    From: jodfishe@silver.ucs.indiana.edu (joseph ...\n",
      "7    From: aldridge@netcom.com (Jacquelin Aldridge)...\n",
      "8    From: geb@cs.pitt.edu (Gordon Banks) Subject: ...\n",
      "9    From: libman@hsc.usc.edu (Marlena Libman) Subj...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#use sort_index()\n",
    "print(X['text'].sort_index()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  category  \\\n",
      "1598   cs.utexas.edu!geraldo.cc.utexas.edu!portal.au...         0   \n",
      "1595   cs.utexas.edu!geraldo.cc.utexas.edu!portal.au...         0   \n",
      "1144   cs.utexas.edu!uunet!olivea!sgigate!sgiblab!ad...         0   \n",
      "1151   cs.utexas.edu!uunet!olivea!sgigate!sgiblab!ad...         0   \n",
      "1506   howland.reston.ans.net!europa.eng.gtefsd.com!...         0   \n",
      "242    howland.reston.ans.net!europa.eng.gtefsd.com!...         0   \n",
      "1426   sgiblab!adagio.panasonic.com!nntp-server.calt...         0   \n",
      "1329   uunet!olivea!sgigate!sgiblab!adagio.panasonic...         0   \n",
      "1096   wupost!uunet!olivea!sgigate!sgi!fido!solntze....         0   \n",
      "1156   wupost!uunet!olivea!sgigate!sgi!fido!solntze....         0   \n",
      "\n",
      "     category_name  \n",
      "1598   alt.atheism  \n",
      "1595   alt.atheism  \n",
      "1144   alt.atheism  \n",
      "1151   alt.atheism  \n",
      "1506   alt.atheism  \n",
      "242    alt.atheism  \n",
      "1426   alt.atheism  \n",
      "1329   alt.atheism  \n",
      "1096   alt.atheism  \n",
      "1156   alt.atheism  \n"
     ]
    }
   ],
   "source": [
    "#use sort_values(). It will sort based on the order of the first alphabet rather than index.\n",
    "print(X.sort_values(by = 'text')[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 5:\n",
    "There is an old saying that goes, \"The devil is in the details.\" When we are working with extremely large data, it's difficult to check records one by one (as we have been doing so far). And also, we don't even know what kind of missing values we are facing. Thus, \"debugging\" skills get sharper as we spend more time solving bugs. Let's focus on a different method to check for missing values and the kinds of missing values you may encounter. It's not easy to check for missing values as you will find out in a minute.\n",
    "\n",
    "Please check the data and the process below, describe what you observe and why it happened.\n",
    "HintHint  : why .isnull() didn't work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer here\n",
    "'''\n",
    "I observed that isnull() function worked for index 0, 1, and 4, while it doesn't work for index 2, 3, and 5. It's because that\n",
    "index 2, 3, and 5 were actually stored some string value. That is, 'NaN', 'None', and ''!!! Therefore, isnull() function wouldn't\n",
    "work for them.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>> Exercise 6 (take home):\n",
    "Notice any changes to the `X` dataframe? What are they? Report every change you noticed as compared to the previous state of `X`. Feel free to query and look more closely at the dataframe for these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
